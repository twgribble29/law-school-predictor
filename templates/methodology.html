{% extends "base.html" %}
{% block title %}Methodology - Law School Predictor{% endblock %}

{% block content %}
<div class="content-page">
    <h1>Methodology</h1>
    <p class="page-subtitle">How our predictions work, what data we use, and what the model can (and can't) tell you.</p>

    <div class="caveat-card">
        <h3>
            <svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" width="20" height="20"><path d="M10.29 3.86L1.82 18a2 2 0 001.71 3h16.94a2 2 0 001.71-3L13.71 3.86a2 2 0 00-3.42 0z"/><line x1="12" y1="9" x2="12" y2="13"/><line x1="12" y1="17" x2="12.01" y2="17"/></svg>
            Data Limitations &amp; Caveats
        </h3>
        <p>All training data is <strong>self-reported</strong> by applicants on LSD.Law. This means:</p>
        <ul>
            <li>Users who report data may not be representative of all applicants&mdash;they tend to skew toward higher-stat, more engaged applicants</li>
            <li>Some outcomes (especially waitlist resolutions and withdrawals) may be underreported</li>
            <li>GPA and LSAT scores are self-reported and occasionally rounded or misremembered</li>
            <li>URM status is self-identified and definitions may vary between users</li>
        </ul>
        <p>Predictions should be treated as <strong>informed estimates</strong>, not guarantees. Use them as one data point among many in your decision-making process.</p>
    </div>

    <h2>Data Source</h2>
    <p>Our models are trained on self-reported admissions data from <strong>Law School Data (LSD.Law)</strong>, spanning the <strong>2003&ndash;2025</strong> admissions cycles. The dataset includes over 100,000 individual application outcomes across 193 ABA-accredited law schools.</p>
    <p>For each application, we use the following information:</p>
    <ul>
        <li><strong>LSAT score</strong> (120&ndash;180)</li>
        <li><strong>GPA</strong> (cumulative undergraduate, to the hundredths place)</li>
        <li><strong>Application timing</strong> (days after September 1 that the application was submitted)</li>
        <li><strong>Softs tier</strong> <span class="info-tip">i<span class="tip-content"><strong>T1:</strong> Exceptional (Rhodes Scholar, Olympic athlete)<br><strong>T2:</strong> Strong (significant leadership)<br><strong>T3:</strong> Average (standard clubs, internships)<br><strong>T4:</strong> Weak (minimal activities)<br><a href="https://www.lsd.law" target="_blank">Learn more at LSD.Law</a></span></span> (self-reported extracurricular/work quality rating, T1&ndash;T4)</li>
        <li><strong>Work experience</strong> (whether the applicant had post-undergrad work experience)</li>
        <li><strong>Years out of undergrad</strong></li>
        <li><strong>URM status</strong> (underrepresented minority)</li>
        <li><strong>Cycle year</strong></li>
        <li><strong>Decisions at other schools</strong> (for the enhanced model)</li>
    </ul>

    <h2>School Selectivity Ranking</h2>
    <p>Each law school is assigned a <strong>selectivity score</strong> derived from an Elo-style rating system based on historical LSAT medians and GPA medians. This score (ranging from ~274 to ~331) captures how competitive a school is and serves as the single most important feature in the model.</p>
    <p>Our selectivity ranking correlates <span class="stat-highlight">0.917</span> with US News law school rankings, providing an objective, data-driven measure of school competitiveness.</p>

    <h2>Model Architecture</h2>
    <p>We train two types of models for each URM category (4 models total):</p>

    <h3>Baseline Model</h3>
    <p>Uses only the applicant's stats and school information. This is the model used when no decisions from other schools are provided. Features include LSAT, GPA, school selectivity, application timing, softs, work experience, years out, cycle year, and three interaction terms (LSAT&times;selectivity, GPA&times;selectivity, timing&times;selectivity).</p>

    <h3>Enhanced Model</h3>
    <p>Adds <strong>decision features</strong> from other schools: number of acceptances, rejections, and waitlists received, along with the average selectivity of those outcomes. This model is used when you provide decisions from other schools, and it captures the signal that "if you were accepted at School X, your chances at School Y also shift."</p>
    <p>Decision features are computed using <strong>leave-one-out</strong> to prevent data leakage&mdash;when predicting for a given school, only decisions from <em>other</em> schools are used.</p>

    <div class="callout-card">
        <h3>
            <svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" width="20" height="20"><circle cx="12" cy="12" r="10"/><line x1="12" y1="16" x2="12" y2="12"/><line x1="12" y1="8" x2="12.01" y2="8"/></svg>
            Understanding Your Enhanced Predictions
        </h3>
        <p>When you add decisions from other schools, you might notice some counterintuitive results. For example, a school that accepted you may still show a probability below 100%. This is normal and expected because:</p>
        <p>The model predicts the probability that a <em>typical applicant with your profile</em> would be accepted&mdash;it doesn't know about your specific acceptance. Your actual outcome is one data point; the model is estimating the underlying probability based on thousands of similar applicants.</p>
        <p>Think of it like a coin flip: if a coin lands heads, you wouldn't say it was a 100% heads coin. Similarly, your acceptance at a school doesn't mean the model should output 100%&mdash;it means you were on the favorable side of the probability distribution.</p>
    </div>

    <h3>Algorithm</h3>
    <p>All models use <strong>HistGradientBoostingClassifier</strong> (scikit-learn), a histogram-based gradient boosting algorithm. Key training improvements:</p>
    <ul>
        <li><strong>Temporal weighting:</strong> Recent admissions cycles are weighted more heavily using exponential decay (0.9<sup>age</sup>), reflecting the significant shift in acceptance rates post-2020.</li>
        <li><strong>Interaction features:</strong> LSAT&times;selectivity, GPA&times;selectivity, and timing&times;selectivity capture how the value of each stat varies by school competitiveness.</li>
        <li><strong>Early stopping:</strong> 500 max iterations with 30-round patience to prevent overfitting.</li>
    </ul>

    <h2>Model Performance</h2>

    <div class="metrics-grid">
        <div class="metric-card">
            <div class="metric-value">0.916</div>
            <div class="metric-label">Non-URM Enhanced<br>Validation AUC</div>
        </div>
        <div class="metric-card">
            <div class="metric-value">0.883</div>
            <div class="metric-label">URM Enhanced<br>Validation AUC</div>
        </div>
        <div class="metric-card">
            <div class="metric-value">0.907</div>
            <div class="metric-label">Median Per-School<br>AUC</div>
        </div>
        <div class="metric-card">
            <div class="metric-value">~2.5pp</div>
            <div class="metric-label">Max Calibration<br>Error</div>
        </div>
    </div>

    <p>The models are well-calibrated: a prediction of 60% means roughly a 58&ndash;63% actual acceptance rate. Calibration was validated across deciles with a maximum error of approximately 2.5 percentage points.</p>

    <h2>Where the Model Struggles</h2>
    <p>No stats-based model can capture the full picture. Our error analysis identified these systematic challenges:</p>
    <ul>
        <li><strong>T6 holistic review:</strong> Yale, Stanford, Harvard, Chicago, Columbia, and NYU use heavy holistic review. The model achieves lower AUC (0.82&ndash;0.84) at these schools because personal statements, recommendations, and institutional priorities matter more.</li>
        <li><strong>Splitter profiles:</strong> Applicants with a high LSAT but low GPA (or vice versa) are hardest to predict (AUC 0.894) because schools weight these differently.</li>
        <li><strong>Unobserved factors:</strong> Legacy status, athletic recruitment, URM misclassification in the data, character &amp; fitness issues, and yield protection are real but not captured.</li>
        <li><strong>Recent cycles:</strong> The 2023&ndash;2024 cycles show slightly lower performance (AUC ~0.88) as admissions become more competitive. Temporal weighting helps but doesn't fully bridge this gap.</li>
    </ul>

    <div class="info-card">
        <p><strong>Bottom line:</strong> This tool provides a strong statistical baseline for your chances. Use it to inform your school list, but remember that holistic factors&mdash;your personal statement, letters of recommendation, and unique circumstances&mdash;matter, especially at top schools.</p>
    </div>

    <h2>What "Not Reported" Means</h2>
    <p>If you select "Not Reported" for softs or leave years out at 0, the model uses default values that represent a typical applicant. This won't significantly affect your LSAT/GPA-driven predictions, as softs and work experience have relatively small feature importance (~0.3% and ~0.1% respectively).</p>

    <h2>Technical Details</h2>
    <ul>
        <li><strong>Framework:</strong> Python, scikit-learn, Flask</li>
        <li><strong>Training data:</strong> ~65,000 non-URM and ~12,000 URM application outcomes</li>
        <li><strong>Validation:</strong> Temporal split (train: 2003&ndash;2021, val: 2022&ndash;2023, test: 2024&ndash;2025)</li>
        <li><strong>Features:</strong> 11 baseline / 20 enhanced</li>
        <li><strong>Temporal decay:</strong> 0.9<sup>(max_year - cycle_year)</sup></li>
        <li><strong>Source code:</strong> <a href="https://github.com/twgribble29/law-school-predictor" style="color: var(--green-500);">GitHub</a></li>
    </ul>
</div>
{% endblock %}
